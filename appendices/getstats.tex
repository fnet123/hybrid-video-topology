\chapter{Extracting \texttt{getStats} data}\label{chp:getstats}

Data extraction from the browsers while running the tests consisted of two parts: A JavaScript file that was injected into the browser extracting the actual data, and a webserver running outside the test cluster storing the captured data for analysis.

Since the data directly from \texttt{getStats} is overly verbose and a bit hard to navigate, some cleanup is performed in the browser before shipping it off to storage, to have as small an impact on the test bandwidth as possible. If no influence of analytics is desired, a local web server could have been spawned on each node running in the test, preventing any external data usage during the duration of the test. The data could then be collected later. This was not considered necessary for these tests, as the overhead of relatively small JSON posted over HTTP is very small compared to streaming live video.

The JavaScript was injected into Chrome using the extension ``Custom JavaScript for websites''\footnote{Available here: \url{https://chrome.google.com/webstore/detail/custom-javascript-for-web/poakhlngfciodnhlhhgnaaelnpjljija}}. The script was configured to dump statistics to the external server every second.

The client-side script is very compact and carries negligible overhead on page load. Minified and gzipped it weighs in at 1.3kB, with no external dependencies.

Since the returned data from the \texttt{getStats} API differs between browsers, it has to be cleaned up before it can be presented for analysis. This could be done either on the client side, or on the server side. I'd argue that this is most sensibly done on the server side, since if keeps the client-side script lighter, prevents it from having to incorporate changes as the browsers change, and reduces the likelihood of crashes. If parsing is done here, the script will have to know about every change done by every browser, which will make it huge and failure-prone when the browsers change the data format. If the script submits the user agent string with the results, the server can collect them all, and when new data appears, it can add handlers for that, and build up support for ever more user agents in the backend.

However, there's a balance to be found. The \texttt{getStats} API is very verbose, returning lots of data of connection candidates and key pairs and lots of data that is generally not that interesting for analytics. To reduce impact on bandwidth, it's of interest to the client to not have to submit a lot of junk that will be discarded by the backend anyway. Thus the script used here tries to find the middle way; it detects the interesting parts (audio in/out, video in/out and current connection details) and discards the rest, sending the interesting parts off to the server without any further parsing of the contents.

Support for Firefox is only half-way there, reports are fetched from the API but the aforementioned detection of report types is not compatible with Firefox. Extending the script to support this should be easy.

\inputminted[
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]
    {javascript}{tools/getstats.js}

The web server code storing the incomng data is quite simple. It performs some very basic sanity checking of the incoming data, and appends it to a file in a customizable location. It uses the Flask python framework\footnote{Available here: \url{http://flask.pocoo.org/}}.

\inputminted[
    linenos,
    numbersep=5pt,
    frame=lines,
    framesep=2mm]
    {python}{tools/statserver.py}
