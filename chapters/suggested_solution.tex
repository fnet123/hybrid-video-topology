\chapter{Suggested solution}
\label{chp:suggested-solution}

In this chapter we'll present our solution to the problem.

\section{Modelling}

With some prior knowledge of graph algorithms, it might be an easy first guess to look at our sample topologies and see a certain similarity to a flow problem -- edges have capacities, there's a set of sources, a set of sinks, and we want to maximize \emph{something}. However, there's a couple of things that make it hard to solve directly as a traditional flow problem, particularly that we don't have a single source and a single sink, but lots of them. Trying to model it as a single-source, single-sink problem quickly leads you to discover the limitations of that model, where you realize that if you actually solved, how would you be able to tell which node is generating flow on a given edge? Clearly, we need a way to distinguish node A's video from node B's video. And we need to make sure that all nodes receive video from every other node, not just maximum bitrate of \emph{any} video.

If we change perspective slightly, we see that this is not a single flow problem, but a \emph{series} of flow problems, sharing an underlying constrained resource. There's the problem of routing video from A to all other nodes, there's the problem of routing video from B to all other nodes, etc. In a conversation with $n$ participants, we now have $n$ separate flow problems, which all share the same resource. However, also this model is too limiting for our use case, as video is sent at a given bitrate, and this stream can neither be split at a given node without incurring a cost, nor does it make sense to add it; two 2Mbit videos cannot be joined to form a 4Mbit video. If you put enough restrictions on your nodes and edges it's probable that you might be able to able to prevent this from happening, but there's another way.\todo{Insert graph showing how this fails. Does it fail it any obvious way? With flow conservation on the nodes it doesn't seem like it, actually. But it might be harder to add restrictions on re-encoding later. Or it's a completely valid option, open for further investigation.}

This time around, imagine that we have a separate flow problem for each \emph{pair} of nodes; we have one problem routing node A's video to node B, we have one problem routing node A's video to node C, etc. This yields a total of $n(n-1)$ problems, which is $O(n^2)$. Modelling the problem this way means we don't have to add any supernodes to connect the targets, each node can act as sink for the video stream, hereafter known as the \emph{commodity}, destined for itself.

One other thing we notice from our initial graph compared to traditional flow problems is that we notice that we have \emph{node constraints} in the initial graph, while flow problems only work on \emph{edge constraints}. To accommodate this, we split each node into two parts, hereafter called the external and internal part, as illustrated in \autoref{fig:node-splitting}.

\todo[inline]{Insert node splitting figure}
\begin{figure}[ht!]
\centering
    \digraph{nodesplittingpre}{
        edge [dir=none];
        rankdir=LR;
        a [label="A (20/5)"];
        b [label="B (30/15)"];

        a -> b [label="8ms"];
        b -> a [label="7ms"];
    }
    \caption{Initial problem}
\end{figure}

\begin{figure}[ht!]
\centering
    \digraph{nodesplittingpost}{
        rankdir=LR;
        aext [label="A ext"];
        aint [label="A int"];
        bext [label="B ext"];
        bint [label="B int"];

        aint -> aext [label="5Mbit"];
        aext -> aint [label="20Mbit"];

        aext -> bext [label="8ms"];
        bext -> aext [label="7ms"];

        bint -> bext [label="15Mbit"];
        bext -> bint [label="30Mbit"];
    }
    \caption{Problem after node splitting}
\end{figure}\todo{[minor]: Make both arrows bendy?}


\subsection{Alternative model}

One alternative way to model the problem, is to skip the node splitting step of the previous model, and instead connect nodes directly, but stay below bandwidth by adding constraints for the total sum going out from each node. Which model to choose is -- as in every engineering matter -- a question of priorities. The first model is a bit harder to comprehend initially, but results in fewer total edges than the latter model does when $n>3$, as can be seen in \autoref{fig:model-scaling}. Edge counts do not matter that much when $n$ is low as finding a solution will occur in trivial time anyway, but the difference might be substantial when n is larger. \todo{If performance ends up being an issue for larger graphs, add a note here about more research being needed into optimizing performance, as alternative models might be an avenue to explore for achieving that.}

\begin{figure}
\centering
    \begin{tikzpicture}
        \begin{axis}[
            xlabel={n},
            ylabel={Number of edges},
            xlabel near ticks,
            ylabel near ticks,
            legend pos=north west,
            xmin=2,
            ymin=1,
            ymax=1000,
            xmax=15]

        \addplot+[dashed, domain=2:15, mark=none, color=blue]{6*x*(x-1)};
        \addlegendentry{$en(n-1), e=6$}

        \addplot+[domain=2:15, mark=none, color=blue]{4*x*(x-1)};
        \addlegendentry{$en(n-1), e=4$}

        \addplot+[dashed, domain=2:15, mark=none, color=red]{x*(x-1) + 2*x*6};
        \addlegendentry{$n(n-1) + 2en, e=6$ }

        \addplot+[domain=2:15, mark=none, color=red]{x*(x-1) + 2*x*4};
        \addlegendentry{$n(n-1) + 2en, e=4$ }
        \label{fig:utility-latency}
        \end{axis}
    \end{tikzpicture}
    \caption{How the number of edges in the graph scales for different modeling techniques.}
    \label{fig:model-scaling}
\end{figure}


\section{The Algorithm}

Given the model, how do you efficiently find the "best" topology?

\section{Tuning}

How can the algorithm be tuned, and how was the parameters chosen?
