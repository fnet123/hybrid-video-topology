\chapter{Discussion}
\label{chp:discussion}

In this chapter we'll take a step back, and try to evaluate both the experiments and the suggested solution from a high level, to see what's necessary to get any further traction.

\section{Experimental Results}

Our experiments were run in browser-homogeneous environments; all browsers in a conversation were the same. It would be interesting to see how the test cases would have evolved in a more realistic scenario, where some of the users are running Firefox and others are running Chrome. Our ``traveller'' test case for example, where one node has severely limited bandwidth and higher latencies than the two others, would make an interesting test. How does Firefox act as the limited node, when the two others are Chrome, compared to how Chrome acts as limited, when the two others are Firefox? As we saw in our results, Chrome is much better at negotiating bitrates that don't saturate a node's link, would this also be the case if the sender is Firefox? Results from a test like that would indicate where exactly Firefox is failing, whether it's not adhering to signals sent by the limited node, or whether the limited node fails to send to appropriate signals.

These sort of cross-browser tests could also provide tremendous value to the browser implementors if they could be run in an automatic way (like discussed in \autoref{subsec:automated-testing}). When tweaking the implementations, the implementors would be able to run a sanity check towards other browsers and itself in a set of test cases, immediately seeing how the changes affect the performance in different test cases.


\section{Implementation and Deployment}

Implementing a script demonstrating how the routing works was harder than expected, and not all desired features could be implemented. As every edge in the flow network is a variable, and since they all will contribute with either cost or gain, you need to keep track of them somehow. The sample script did this

It must be noted, our custom algorithm will not be unjustly favored since it can access the additional forwarding nodes. Google Hangouts and Skype both utilize similar nodes, but their architecture and inner working is hidden from us due to the proprietary nature of these products, and thus they can not be modelled accurately. Only pure peer-to-peer WebRTC products like appear.in and Firefox Hello rely only on the client nodes, but they're both free to add any helpers themselves.


\section{Finding Network Properties}

The suggested solution is heavily reliant on having a fairly accurate representation of the actual capabilities of the network in a given conversation. Some of the properties we're looking for are relatively easy to establish, like latency between the nodes. If the service yields IP-addresses of all the participants when a new user joins, the user can ping those addresses and determine latency to all of them in hundreds of milliseconds.

Bandwidth is notoriously trickier. The biggest problem for measuring bandwidth is that in general it takes a long time, something most users would be very annoyed if they had to do for every conversation. However, the suggested solution does not require you to know your exact bandwidth, and underestimates can work pretty well. Thus a rough test against the provider upon entering a conversation should yield usable results, but this is largely a hypothesis. More research into the effect of accurate bandwidth measurements would be needed to say anything conclusively on the matter.

While tricky, I do assume that efficient ways of gathering this information can be found.


\section{Who's the Boss}

In theory, any node in the conversation could model the problem and solve it to derive where its video should be routed and at what quantity. However, no node can do this without full knowledge of the flow network, which requires everyone to share data with everyone. You'd also need to make sure that everyone in the conversation agrees about the numbers the node came up with, which is hard in a distributed scenario. Since most solutions will be require some central provider to find contacts, it's natural to assume that the service provider can be the negotiator. As the intended application is for WebRTC deployments, this keeps the client light and allows the LP-solver to run where there's most compute capacity available.

This is not a hard requirement however, WebRTC has no dependency on centralization to work. The only thing WebRTC requires is an external channel to communicate session establishment. Practically all WebRTC-based video conferencing deployments do this over HTTP towards a central server provided by the service, but WebRTC could also be utilized without any providers, if the communicating parties publish connection info over external channels, like Twitter or mail. The proposed solution could work in such a scenario, perhaps by letting the node with the most compute capacity solve the LP-problem. Such distributed schemes have not been the focus for this thesis, but is an interesting avenue for further research.


\section{Limitations}

Scalability.

As long as conversations are modelled as flow networks, any device or network characteristic that can be included in such a model and described as a linear function or approximation can be used to define the objective function. Our goal of maximum bandwidth at minimum latency could also have been minimum latency at minimum server costs, or minimal CPU usage at maximum bandwidth. Any combination is possible.

Do we need LP?

\todo[inline]{Find upper limits for performance of the solution. How big networks?}


\section{Privacy}

For privacy-oriented consumers, the suggested solution opens up an interesting opportunity. Most of the existing solutions, like Google Hangouts and Skype, route every conversation through company-controlled data centers, which in light of the recent NSA revelations\footnote{I'm referring here to the PRISM program revealed by Edward Snowden \cite{prism}} is less than stellar from a privacy perspective. Dynamic topologies like discussed in this thesis, combined with a multitude of global VPS providers which provide quick-bootable disk images, enable consumers to provide their own infrastructure that can be used to offload their compute and connectivity to. Combined with WebRTC's mandatory encryption using DTLS, this makes conversations private and not susceptible to surveillance, from companies or governments.

For providers, this could be great news. They would not have to provide expensive infrastructure for call routing, and would rather focus on interfaces for finding friends and other complementary features like text chat, file sharing, call history, contact lists, etc.

This would make the market more volatile, as there would be a lower investment barrier to be able provide a full-blown communication infrastructure. New providers with original ideas could quickly blossom, as users traditionally have not shown a lot of loyalty to most communications platforms\footnote{We've seen this several times recently, as users have migrated en masse from SMS to Facebook Messenger to Whatsapp to Snapchat. The only value a user sees in their provider seem to be whether they can reach their friends through them.}. Providing quick-bootable user-controlled server images could be a potential for innovation for companies. It boils down to where the user decides to put their trust, a powerful desktop at home could be used as a relay, or a cloud VPS could provide the same service, granted that you trust the company that provides them -- which does not have to be the same company that provides the room.

This author believes this decentralizes and opens up communication, which is more in line with the Internet spirit than the current walled-garden solutions.


\section{Dynamic Conversations}\label{sec:dynamic-conversations}

In our solution, we have so far assumed that most of the properties in the network are static, like nodes, upload/download capacities, latencies, and possibly available CPU, if implemented. In reality however, many of these properties are likely to fluctuate during a conversation, either because the people join and leave conversations, users might be multi-tasking and running other IO-intensive applications on the nodes, or mobile users might have started a conversation over WiFi at home, but started walking to work and thus changed to a cellular connection mid-conversation.

Regularly assessing the state of the conversation should be a natural extension of the system, and should not pose to large of a challenge for implementations. Push-based notifications should also be able to trigger a re-assessment, such as the room notifying of new nodes joining, to avoid any delay for events that everyone in a conversation should react to immediately. As the algorithm underlying most LP solvers, Simplex, is iterative in nature, it might be possible for a solution to a new configuration to start from a previous known good solution, and thus only incorporating the changes to the system to avoid a full re-computation of the topology, but this has not been considered a goal of a system in this first exploration of the idea.

The biggest challenge is ensuring the transitions between topologies become transparent to the user, but as users are more lenient with lagging video than lagging audio, audio could be duplicated on several topologies before video is switched. This could help audio run smoothly during the transition, while video might take a few moments to catch up.
