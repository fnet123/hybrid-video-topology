\chapter{Experiments}\label{chp:experiments}

In this chapter we'll benchmark a WebRTC-based video conferencing solution with our test cases, to get a sense of how a peer-to-peer architecture performs. The results from these experiments will be used as motivation for the approach outlined in \autoref{chp:suggested-solution}.


\section{Test Setup}

To benchmark appear.in, our WebRTC-based video conferencing solution of choice, we have utilized a small cluster of desktop computers with web cameras, running the most recent versions of Mozilla Firefox\footnote{Version 36.0.4, latest version as of 2015.06.05 when the tests were run} and Google Chrome\footnote{Version 41.0.2272.101, latest version as of 2015.06.11 when the tests were run}. These two browsers were chosen since they collectively represent 85\% of the browser market (according to both appear.in data as seen in \autoref{chp:appear.in-usage} and the W3C \cite{browser-stats}), and are powered by two different underlying engines. The goal of the benchmark is to get a sense of how the browsers -- and by extension, appear.in -- performs with regard to latency and bandwidth usage in our different test scenarios, and to observe how resources are shared among the nodes in a conversation.

Since the test covers two different browsers which do not share a common API (more on this later), measurements were done in two different ways. For Firefox, which do not expose timing data of WebRTC-streams, a browser-external way of measuring end-to-end latencies was necessary. For this we synchronized all the clocks in the cluster to the same \gls{NTP} server, and set another independent node -- also synced to the same time server -- to run a timer. Each of the nodes in the test filmed this timer, and with the same timer running locally in a terminal, the end-to-end latency could be extracted by taking regular screenshots, and finding the difference between the local timer and the timer as sent by the other nodes. See \autoref{fig:example-screenshot} for an example of how the screenshots looked.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{example-screenshot}
    \caption{An example screenshot from a Firefox test run on node A. The nodes are, from top left and clockwise, A, C, B and D. We can see how the $\approx$33 ms refresh rate manifests itself, as the visible times are .928, .959, .990, and 0.021.}
    \label{fig:example-screenshot}
\end{figure}

Bandwidth usage was measured by running \texttt{tcpdump} throughout the test run, and bitrates between each pair of nodes was extracted with \texttt{tshark}.

For the Chrome tests, this was a bit simpler and less manual, as Chrome provides both timing data and bitrates through the \texttt{getStats} API. Firefox also supports \texttt{getStats}, but does not include timing data, even though the data is assumed to be available internally in the browser. Data was extracted from Chrome using the scripts included in \autoref{chp:getstats}, and submitted to an external server collecting the data from all nodes.


\subsection{Sampling}

At the start of the test, the nodes join the conversation in alphabetical order (the node names are the letters A-G), as soon as the previous node has established connection to all the other parties already in the conversation. Preferably the join order would be random and the results averaged over several test runs, but due to time constraints this was not possible.

When all nodes have established bi-directional connections, the conversation was left running for a minute, before sampling started. This was done to allow some time to reach a stable state.

For Firefox, where the interpretation of the results is a very tedious and laboursome process, samples were taken every $\approx$12 s.\footnote{A bit variable, as it's 10 s + the delay for taking and storing the screenshot.} The last six samples for each node was interpreted and stored, yielding a total sample time of $\approx$80 s. On Chrome, where there's no interpretation step, samples were submitted every second. The sample time was two minutes, yielding 120 samples for each node.

For all test cases the test was first run without any traffic shaping applied, so to see that the browsers behave as expected in an unconstrained setting. The full data set from these tests are included in \autoref{chp:no-shaping-comp}. Both browsers behaved as expected, which helps validate that the results presented here are regressions because of the constraints applied, and not CPU or other factors not controlled in the experiment.


\subsection{getStats}

The relevant values offered by the \texttt{getStats}-API on Chrome\footnote{Documentation is very poor for the \texttt{getStats}-API, as the specification is not completed yet, so the most reliable reference is the source: \url{https://chromium.googlesource.com/external/webrtc/+/master/talk/app/webrtc/statstypes.cc}} and Firefox is presented in \autoref{tab:incoming-video} and \autoref{tab:outgoing-video}. The values reported here were dumped from what's returned by the browser.

\begin{center}
    \captionof{table}{Incoming video data}
    \label{tab:incoming-video}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Chrome} & \textbf{Firefox} \\ \hline
        \multicolumn{2}{| c |}{bytesReceived:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsLost:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsReceived:\texttt{str/int}} \\ \hline
        googCurrentDelayMs:\texttt{str} & jitter:\texttt{float} \\
        googDecodeMs:\texttt{str} & mozRtt:\texttt{int} \\
        googJitterBufferMs:\texttt{str} & \\
        googMaxDecodeMs:\texttt{str} & \\
        googMinPlayoutDelayMs:\texttt{str} & \\
        googRenderDelayMs:\texttt{str} & \\
        googTargetDelayMs:\texttt{str} & \\ \hline
    \end{tabular}
\end{center}

\begin{center}
    \captionof{table}{Outgoing video data}
    \label{tab:outgoing-video}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Chrome} & \textbf{Firefox} \\ \hline
        \multicolumn{2}{| c |}{bytesSent:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsSent:\texttt{str/int}} \\ \hline
        googAvgEncodeMs:\texttt{str} & bitrateMean:\texttt{float} \\
        googCaptureJitterMs:\texttt{str} & bitrateStdDev:\texttt{float} \\
        googCaptureQueueDelayMsPerS:\texttt{str} & droppedFrames:\texttt{int} \\
        googCodecName:\texttt{str} & framerateMean:\texttt{float} \\
        googBandwidthLimitedResolution:\texttt{str} & framerateStdDev:\texttt{float} \\
        googCpuLimitedResolution:\texttt{str} & \\
        googViewLimitedResolution:\texttt{str} & \\
        googRtt:\texttt{str} & \\
        packetsLost:\texttt{str} & \\ \hline
    \end{tabular}
\end{center}

It's a sad to see that all of the values are casted to strings in Chrome. This is not the case on Firefox, where appropriate types are used. As we also see, all of the timing-related values we're interested in are vendor-prefixed on Chrome, which hints to their unspecified nature. Note that both browsers report more data than what is shown here, this is only the data I consider to be relevant for link quality measurements. Chrome is very helpful in providing why resolution is limited\footnote{Although I would prefer to see a single value ``limitedResolution'', which could be either \texttt{false}, \texttt{``cpu''}, \texttt{``bandwidth''} or \texttt{``view''}, to make it a bit less verbose and easier to extend.} (received resolution is present in the full data set), which could be incorporated into more advanced models. When the \texttt{getStats} API specification\footnote{\url{http://w3c.github.io/webrtc-stats/}} reaches stable in the W3C, I expect most of these differences to disappear. Note that Firefox has the API closest to the proposed specification as of the time of writing.

The values of jitterBufferMs, renderDelayMs, decodeMs and currentDelayMs was summed to get the observed latency. This was based on some trial and error to see what best aligned with the observed latencies using the timer, as outlined in \autoref{chp:getstats-vs-timer}, since they are not documented anywhere. A more thorough reading of the source code might reveal a more accurate combination, but there was no time to do this for this thesis.


\subsection{Constraining Nodes}

To configure the cluster according to the different test cases, we utilized the Linux traffic control utility \texttt{tc}, which is capable of rate-limiting incoming and outgoing traffic, as well as delaying traffic destined for certain hosts. A small script was developed to act as a glue layer between a representation of a network and \texttt{tc}, making configuration repeatable and easily parametrized. The script is included in \autoref{chp:apply-case}. The test cases from \autoref{chp:test-cases} were serialized into \gls{yaml}, and the same case definitions could then be used by both the script configuring the nodes, and for the sample solution provided in \autoref{chp:suggested-solution}.

Applying a given test case is thus completely independent of the actual network utilized in the test cluster, keeping all intelligence on the nodes themselves. This removed the need for expensive routers or having to customize the application code, thus making the method application-agnostic and applicable to any peer-to-peer solution, not just appear.in.


\subsection{Automated Testing?}\label{subsec:automated-testing}

Ideally, testing would be automated and not require running a graphical environment, to allow tests to be run often and in response to events such as commits. This could be possible using by running a browser in a fake framebuffer like \texttt{Xvfb}\footnote{\url{http://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml}} and faking out a media stream\footnote{Chrome: \texttt{-{}-use-fake-device-for-media-stream}, Firefox: \texttt{getUserMedia(\{fake: true, <...>\})}. More info about this approach can be found at \url{http://images.tmcnet.com/expo/webrtc-conference/presentations/san-jose-14/D3-2_Testing_v2_2.pdf}}. Both browsers should be able to be tested in such a setting, but data would be limited to what can be extracted through the \texttt{getStats}-API as described above. Still, it's better than nothing. It is thus possible to automate this, but was considered out of scope for this thesis.

Chrome runs regular interoperability tests with Firefox\footnote{Google blogged about this: \url{http://googletesting.blogspot.se/2014/09/chrome-firefox-webrtc-interop-test-pt-2.html}}, but these tests only test that calls can be established, and do not test any network configurations or measure statistics. Integrating the results from this thesis into this test suite is encouraged for more insight into the performance and behavior of WebRTC implementations. The W3C also maintain a test suite for implementations\footnote{\url{http://www.webrtc.org/testing/w3c-conformance-tests}}, but those only test compatibility with the APIs, and not network behavior.


\subsection{Caveats}

The Firefox method is accurate in the sense that latencies observed are the actual end-to-end latencies that users would observe, but the precision of the timing values observed is not on the millisecond level we'd prefer. This is due to a number of factors, most notably the refresh rate of the screen running the timer and the frame rate of the video, limiting the precision to $1 s/60\approx17 ms$ and $1 s/30\approx33 ms$ respectively. However, we can surpass this precision by averaging several samples taken during the test run, which is why we take six screenshots for each case. The standard deviation of the measurements is reported in the graphs included later in this chapter, which should give some indication towards how accurate the average is. The sample size is very small and should thus be taken with a grain of salt, but it was the best option available at the time.

Taking several samples to improve accuracy leads us to another weakness, which is the manual interpretation of the screenshots. Due to the frequency-related issues discussed above, many of the images include timestamps that are blurred, as the camera captured two underlying screen updates in the same frame, as shown in \autoref{fig:blurred-capture}.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{blurred-capture}
    \caption{A screenshot where a node has sent two overlaying timestamps. In this case interpreted as 10.106, which is reasonable as it's close to the expected $\approx$33 ms increase from the previous 10.075.}
    \label{fig:blurred-capture}
\end{figure}

In general for these cases, the recorded timestamp was consistently interpreted to be the latest of what could be distinguished in the screenshot.

Even assuming that the timestamps are comprehensible and fairly accurate, there's still a possibility of human error when lots of numbers has to be recorded in this way. To minimize the risk of any mistyped numbers making it into the dataset, any observation outside 1.5 standard deviations of the mean (a range which should include 87\% of the numbers observed) was re-interpreted to verify. There's still a chance of smaller errors making it into the dataset, but we assume that these are small enough and distributed evenly among the nodes to not significantly influence any conclusions drawn.

As not enough cameras of any single model was available for the experiments, two different models\footnote{HP Webcam HD-4110 and Tandberg (now Cisco) PrecisionHD} had to be used. These had slightly different performance characteristics; the cameras were put side by side with a timer and showed a mean difference of 39.6 ms, but with a relatively large standard deviation of 19.5 ms. As the same effects related to refresh rates as discussed above applies, pretty much all the samples were at a 30 ms or 60 ms interval of each other.\footnote{Out of 20 samples, 1 was 0 ms, 13 were 30 ms, 5 were 60 ms, and 1 was 90 ms. Which really means that the sample is in the range of 0--29 ms difference, 30--60 ms difference, and so on.} As the difference was assumed to be normally distributed, the mean was simply added to all measurements from the slower camera model to compensate.

For measuring bandwidth utilization between nodes, our method of using \texttt{tcpdump} is not entirely satisfactory, as there's no way to report actual \emph{consumed} bandwidth by the application. This is because the traffic control features of the Linux kernel lies above \texttt{libpcap}, the library that performs packet capture for \texttt{tcpdump} in the network stack. Effectively this means that any incoming bandwidth reported by \texttt{libpcap} will be before the rate limiting performed by \texttt{tc}. Thus, \texttt{tcpdump} cannot report the actual bandwidth consumed by application, only what was received by the network interface. Nonetheless, the bandwidth \emph{sent} by each node is what was actually sent by the application, but there's no guarantee that the receiver was capable of consuming it all. This is good enough for us though, as we can aggregate the data sent by all nodes to determine how saturated a given node's network link is.

While the method itself is application-agnostic, configuring nodes the way we do is not suitable for testing other architectures, such as the ones used by Hangouts and Skype. This is unfortunate, as a performance comparison between the different architectures would have been very interesting, but without running a local instance of the architecture under test, there's no way to achieve the inter-node latencies we desire. This follows from observing that if node A sends its video stream to a Google server, there's no way for it to signal to Google that when the stream is broadcast to nodes B and C, B's stream should be delayed by $x$ ms, and the stream to C should be delayed $y$ ms. It's also not possible for B and C to apply this latency on the receiving side, as they'd have to split the incoming stream for Google into separate streams for each of the transmitting nodes, which would require both getting access to the DTLS keys used by the web browser to encrypt the traffic, and being capable of splitting the stream and rejoining it again without interfering with the browser.

For the most accurate comparison of bitrate, it would have been preferable to use the same method for sampling this on both browsers. However, as Firefox was incapable of delivering timing data, the \texttt{getStats} API was discarded entirely, even though it could have been used to sample bitrates as observed by the application. This is unfortunate, but the tools left behind by these experiments allow others who want to repeat the tests to not do this mistake.


\section{Results}

\subsection{How To Read the Graphs}

As there will be a lot of graphs in this chapter, a good understanding of how to read them is essential. \autoref{fig:graph-tutorial} gives you a quick primer.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{graph-tutorial}
    \caption{How to read bandwidth graphs. Latency graphs are the same, just different units.}
    \label{fig:graph-tutorial}
\end{figure}

For each node on the x-axis, if it's a latency chart the node's happier the more of the latencies are low. If it's a bitrate chart, the node's happier the more of the bars are high.

Before we embark on the test cases, we put our two sampling methods up against each other, to see whether the results are comparable. For each of the conversation sizes (3, 4, and 7), the results were equal across the board for both browsers. Both had a consistent stable bandwidth at slightly less than 2.1 Mbps, and latencies below 200 ms (Chrome slightly less). The results are included in \autoref{chp:getstats-vs-timer}.

\input{chapters/experiments-traveller}

\input{chapters/experiments-standup}

\input{chapters/experiments-friends}

