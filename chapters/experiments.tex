\chapter{Experiments}\label{chp:experiments}

In this chapter we'll benchmark a WebRTC-based video conferencing solution for our test cases, to get a sense of how a peer-to-peer architecture performs.


\section{Test Setup}

To benchmark appear.in, our WebRTC-based video conferencing solution of choice, we have utilized a small cluster of desktop computers with web cameras, running the most recent versions of Mozilla Firefox\footnote{Version 36.0.4, latest version as of 2015.06.05 when the tests were run} and Google Chrome\footnote{Version 41.0.2272.101, latest version as of 2015.06.11 when the tests were run}. These two browsers were chosen since they collectively represent 85\% of the browser market (according to both appear.in data as seen in \autoref{chp:appear.in-usage} and the W3C \cite{browser-stats}), and are powered by two different underlying engines. The goal of the benchmark is to get a sense of how the browsers -- and by extension, appear.in -- performs with regard to latency and bandwidth usage in our different test scenarios, and from that data observe how resources are shared among the nodes in a conversation.

Since the test covers two different browsers which do not share a common API (more on this later), measurements were done in two different ways. For Firefox, which do not expose timing data of WebRTC-streams, a browser-external way of measuring end-to-end latencies was necessary. For this we synchronized all the clocks in the cluster to the same \gls{NTP} server, and set another independent node -- also synced to the same time server -- to run a timer. Each of the nodes in the test filmed this timer, and with the same timer running locally in a terminal, the end-to-end latency could be extracted by taking regular screenshots, and finding the difference between the local timer and the timer as sent by the other nodes. See \autoref{fig:example-screenshot} for an example of how the screenshots looked.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{example-screenshot}
    \caption{An example screenshot from a test run on node A. The nodes are, from top left and clockwise, A, C, B and D. We can see from the difference between the local timestamp and A's own video stream that there's about 75ms of processing time in the browser before the image is sent (if we round A up to the almost visible 50.021 timestamp), and that the latencies from the other nodes are 31ms, 93ms and 93ms for B, C, and D, respectively.}
    \label{fig:example-screenshot}
\end{figure}

Bandwidth usage was measured by running \texttt{tcpdump} throughout the test run, and later extracting sent and received data with \texttt{tshark}.

For the Chrome tests, this was a bit simpler and less manual, as Chrome do provider timing data through the \texttt{getStats} API. Firefox also supports \texttt{getStats}, but does not return as much data as Chrome does, even though the data is assumed to be available internally in the browser.


\subsection{Sampling}

At the start of the test, the nodes join the conversation in alphabetical order (the node names are letters A-G), as soon as the previous node has established connection to all the other parties already in the conversation. Preferably the join order would be random and the results averaged over several test runs, but due to time constraints this was not possible.

When all nodes have established bi-directional connections, the conversation was left running for a minute, before sampling started. This was done to allow some time to reach a stable state.

For Firefox, where the interpretation of the results is a very tedious and laboursome process, samples were taken every $\approx$12s\footnote{A bit variable, as it's 10s + the delay for taking and storing the screenshot}. The last six samples for each node was interpreted and stored, yielding a total sample time of $\approx$80s. On Chrome, were there's no interpretation step, samples were submitted every second, and the sample time was two minutes, yielding 120 samples for each node.

For all test cases the test was first run without any traffic shaping applied, so that the actual test results could be seen in comparison to results without any network limitations.


\subsection{getStats}

The relevant values offered by the \texttt{getStats}-API on Chrome\footnote{Documentation is very poor for the \texttt{getStats}-API, as the specification is not completed yet, so the most reliable reference is the source: \url{https://chromium.googlesource.com/external/webrtc/+/master/talk/app/webrtc/statstypes.cc}} and Firefox is presented in \autoref{tab:incoming-video} and \autoref{tab:outgoing-video}. The values reported here were dumped from what's returned by the browser.

\begin{center}
    \captionof{table}{Incoming video data}
    \label{tab:incoming-video}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Chrome} & \textbf{Firefox} \\ \hline
        \multicolumn{2}{| c |}{bytesReceived:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsLost:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsReceived:\texttt{str/int}} \\ \hline
        googCurrentDelayMs:\texttt{str} & jitter:\texttt{float} \\
        googDecodeMs:\texttt{str} & mozRtt:\texttt{int} \\
        googJitterBufferMs:\texttt{str} & \\
        googMaxDecodeMs:\texttt{str} & \\
        googMinPlayoutDelayMs:\texttt{str} & \\
        googRenderDelayMs:\texttt{str} & \\
        googTargetDelayMs:\texttt{str} & \\ \hline
    \end{tabular}
\end{center}

\begin{center}
    \captionof{table}{Outgoing video data}
    \label{tab:outgoing-video}
    \begin{tabular}{| l | l |}
        \hline
        \textbf{Chrome} & \textbf{Firefox} \\ \hline
        \multicolumn{2}{| c |}{bytesSent:\texttt{str/int}} \\
        \multicolumn{2}{| c |}{packetsSent:\texttt{str/int}} \\ \hline
        googAvgEncodeMs:\texttt{str} & bitrateMean:\texttt{float} \\
        googCaptureJitterMs:\texttt{str} & bitrateStdDev:\texttt{float} \\
        googCaptureQueueDelayMsPerS:\texttt{str} & droppedFrames:\texttt{int} \\
        googCodecName:\texttt{str} & framerateMean:\texttt{float} \\
        googBandwidthLimitedResolution:\texttt{str} & framerateStdDev:\texttt{float} \\
        googCpuLimitedResolution:\texttt{str} & \\
        googViewLimitedResolution:\texttt{str} & \\
        googRtt:\texttt{str} & \\
        packetsLost:\texttt{str} & \\ \hline
    \end{tabular}
\end{center}

It's a bit sad to see that all of the values are casted to strings in Chrome. This is not the case on Firefox, where appropriate types are used. As we also see, all of the timing-related values we're interested are vendor-prefixed on Chrome, which hints to their unspecified nature. Note that both browsers report more data than what is shown here, this is only the data I consider to be relevant for our bandwidth and latency measurements (and link quality, thus the ones related to packet loss). Chrome is also very helpful in providing why resolution is limited (received resolution is present in the full data set), which could be incorporated into more advanced models.


\subsection{Constraining Nodes}

To configure the cluster according to the different test cases, we utilized the Linux traffic control utility \texttt{tc}, which is capable of rate-limiting incoming and outgoing traffic, as well as delaying traffic destined for certain hosts. A small script was developed to act as a glue layer between a representation of a network and \texttt{tc}, making configuration repeatable and easily parametrized. The script is included in \autoref{chp:apply-case}.

Thus, applying a given test case is completely independent of the actual network utilized in the test cluster, keeping all intelligence on the nodes themselves. This removed the need for expensive routers or having to customize the application code, thus making the method application agnostic and applicable to any peer-to-peer solution, not just appear.in.


\subsection{Automated Testing?}\label{subsec:automated-testing}

Ideally, testing would be automated and not require a running graphical environment, to allow tests to be run often and in response to events such as commits. This could be possible using by running a browser in a fake framebuffer like \texttt{Xvfb}\footnote{\url{http://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml}} and faking out a media stream\footnote{Chrome: \texttt{-{}-use-fake-device-for-media-stream}, Firefox: \texttt{getUserMedia(\{fake: true, <...>\})}. More info about this approach can be found at \url{http://images.tmcnet.com/expo/webrtc-conference/presentations/san-jose-14/D3-2_Testing_v2_2.pdf}}. Both browsers should be able to be tested in such a setting, but data would be limited to what can be extracted through the \texttt{getStats}-API as described above. Still, it's better than nothing. It is thus possible to automate this, but was considered out of scope for this thesis.


\subsection{Caveats}

The Firefox method is accurate in the sense that latencies observed are the actual end-to-end latencies that users would observe, but the precision of the timing values observed is not on the millisecond level we'd prefer. This is due to a number of factors, most notably the refresh rate of the screen running the timer and the framerate of the video, limiting the precision to $1s/60\approx17ms$ and $1s/30\approx33ms$ respectively. However, we can surpass this precision by averaging several samples taken during the test run, which is why we take several screenshots for each test case, once every 10s for at least a minute. The standard deviation of the measurements is reported in the graphs included later in this chapter, which should give some indication towards how accurate the average is.

Taking several samples to improve accuracy leads us to another weakness, which is the manual interpretation of the screenshots. Due to the frequency-related issues discussed above, many of the images include timestamps that are blurred, as the camera captured two underlying screen updates in the same frame, as shown in \autoref{fig:blurred-capture}.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{blurred-capture}
    \caption{A screenshot where a node has sent two overlaying timestamps. In this case interpreted as 10.106, which is reasonable as it's close to the expected 33ms increase from the previous 10.075.}
    \label{fig:blurred-capture}
\end{figure}

In general for these cases, the recorded timestamp was consistently interpreted to be the latest of what could be distinguished in the screenshot.

Even assuming that the timestamps are comprehensible and fairly accurate, there's still a possibility of human error when lots of numbers has to be recorded in this way. To minimize the risk of any mistyped numbers making it into the dataset, any number outside 1.5 standard deviations of the mean (a range which should include 87\% of the numbers observed) was verified once more. There's still a chance of smaller errors making it into the dataset, but we assume that these are small enough and distributed evenly among the nodes to not significantly influence any conclusions drawn.

As not enough cameras of any single model was available for the experiments, two different models\todo{Insert picture of the cameras here, or footnote} had to be used. These had slightly different performance characteristics; the cameras were put side by side with a timer and showed a mean difference of 39.6ms, but with a relatively large standard deviation of 19.5ms. As the same restrictions on screen refresh rates as discussed above, pretty much all the samples were at a 30ms or 60ms interval of each other.\footnote{Out of 20 samples, 1 was 0ms, 13 were 30ms, 5 were 60ms, and 1 was 90ms. Which really means in the range of that frame, that is in the range of 0--29ms difference, 30--60ms difference, and so on.} As the difference was assumed to be normally distributed, the mean was simply added to all measurements from the slower camera model to compensate.

For measuring bandwidth utilization between nodes, our method of using \texttt{tcpdump} is not entirely satisfactory, as there's no way to report actual \emph{consumed} bandwidth by the application. This is because the traffic control features of the Linux kernel lies above \texttt{libpcap}, the library that performs packet capture for \texttt{tcpdump} in the network stack. Effectively this means that any incoming bandwidth reported by \texttt{libpcap} will be before the rate limiting performed by \texttt{tc}. Thus, \texttt{tcpdump} cannot report the actual bandwidth consumed by application, only what was received by the network interface. Nonetheless, the bandwidth \emph{sent} by each node is what was actually sent by the application, but there's no guarantee that the receiver was capable of consuming it all. This is good enough for us though, as we can aggregate the data sent by all nodes to determine how saturated a given node's network link is.

While the method itself is application-agnostic, configuring nodes the way we do is not suitable for testing other architectures, such as the ones used by Hangouts and Skype. This is unfortunate, as a performance comparison between the different architectures would have been very interesting, but without running a local instance of the architecture under test, there's no way to achieve the inter-node latencies we desire. This follows from observing that if node A sends her video stream to a Google server, there's no way for her to signal to Google that when the stream is broadcast to nodes B and C, B's stream should be delayed by $x$ ms, and the stream to C should be delayed $y$ ms. It's also not possible for B and C to apply this latency on the receiving side, as they'd have to split the incoming stream for Google into separate streams for each of the transmitting nodes, which would require both getting access to the DTLS keys used by the web browser to encrypt the traffic, and being capable of splitting the stream and rejoining it again without interfering with the browser.

For the most accurate comparison of bitrate, it would have been preferable to use the same method for sampling this on both browsers. However, as Firefox was incapable of delivering timing data, the \texttt{getStats} API was discarded entirely, even though it could have been used to sample bitrates as observed by the application. This is unfortunate, but the tools left behind by these experiments allow others who want to repeat the tests to not do this mistake.


\section{Results}

\subsection{How To Read the Graphs}

As there will be a lot of graphs in this chapter, a good understanding of how to read them is essential. \autoref{fig:graph-tutorial} gives you a quick primer.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{graph-tutorial}
    \caption{How to read bandwidth graphs. Latency graphs are the same, just different units.}
    \label{fig:graph-tutorial}
\end{figure}

For each node on the x-axis, if it's a latency chart the node's happier the more of the latencies are low. If it's a bitrate chart, the node's happier the more of the bars are high.

Before we embark on the test cases, we put our two sampling methods up against each other, to see whether the results are comparable. For each of the conversation sizes, 3, 4, and 7, the results were equal across the board for both browsers. Both had a consistent stable bandwidth at slightly less than 2.1Mbps, and latencies below 200ms (Chrome slightly less). The results are included in \autoref{no-shaping-comp}.

\todo[inline]{[minor] If time, define custom color set for graphing, based possibly either on solarized or http://clrs.cc/}

\todo{Write tool to extract bandwidth utilization up and down from cases.yml and the latexified results.}

\input{chapters/experiments-traveller}

\input{chapters/experiments-standup}

\input{chapters/experiments-friends}

