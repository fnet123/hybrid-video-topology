\chapter{Experiments}\label{chp:experiments}

In this chapter we'll benchmark a WebRTC-based video conferencing solution for our test cases, to get a sense of how a peer-to-peer architecture performs.


\section{Test Setup}

To benchmark appear.in, our WebRTC-based video conferencing solution of choice, we have utilized a small cluster of desktop computers with web cameras, running a recent version of Mozilla Firefox. The goal of the benchmark is to get a sense of how the application performs with regard to latency and bandwidth usage in our different test scenarios, and from that data observe how resources are shared among the nodes in a conversation.

WebRTC doesn't expose any \glspl{api} for accessing latency of a given stream, thus we had to improvise how to measure this. We settled for a solution based on having each node broadcast a timestamp in its video stream, which each receiving node could then compare to their local time. As long as all the clocks are syncronized against the same NTP servers, they should be more than accurate enough to measure the different latencies observed in the system.

Practically this was accomplished by having each computer film a timer running on a separate computer, and run the same timer locally. A screenshot was taken regularly through the test run, enabling extraction of the time as sent by each of the other nodes later. See \autoref{fig:example-screenshot} for an example of how the screenshots looked.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{example-screenshot}
    \caption{An example screenshot from a test run on node A. The nodes are, from top left and clockwise, A, C, B and D. We can see from the difference between the local timestamp and A's own video stream that there's about 75ms of processing time in the browser before the image is sent (if we round A up to the almost visible 50.021 timestamp), and that the latencies from the other nodes are 31ms, 93ms and 93ms for B, C, and D, respectively.}
    \label{fig:example-screenshot}
\end{figure}

Bandwidth usage was measured by running \texttt{tcpdump} throughout the test run, and later extracting sent and received data with \texttt{tshark}.

To configure the cluster according to the different test cases, we utilized the Linux traffic control utility \texttt{tc}, which is capable of rate-limiting incoming and outgoing traffic, as well as delaying traffic destined for certain hosts. A small script was developed to act as a glue layer between a representation of a network and \texttt{tc}, making configuration repeatable and easily parametrized. The script is included in \autoref{chp:apply-case}.

Thus, applying a given test case is completely independent of the actual network utilized in the test cluster, keeping all intelligence on the nodes themselves. This removed the need for expensive routers or having to customize the application code, thus making the method application agnostic and applicable to any peer-to-peer solution, not just appear.in.

In contrast to most other video conferencing solutions, appear.in displays a node's own video stream just as big as the other streams in the conversation. This enables us to get a sense of how much time the browser spends processing the incoming video stream, as we can estimate this as the difference between the local timestamp and the timestamp of your own video stream as displayed in the browser. Since we're not benchmarking CPU usage in these tests, we can subtract this difference from the observed latencies between the other nodes, to only look at latencies originating in the network stack. Hence, all latencies reported in this thesis are lower than the actual observed end-to-end latencies by 50--70ms.

Ideally, testing would be automated and not require a running graphical environment, to allow it to be tested using remote servers. This could be possible using \texttt{node-webrtc}\footnote{\url{https://github.com/js-platform/node-webrtc}} or running a browser in a fake framebuffer like \texttt{Xvfb}\footnote{\url{http://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml}} and faking out a media stream\footnote{Chrome: \texttt{-{}-use-fake-device-for-media-stream}, Firefox: \texttt{getUserMedia(\{fake: true, <...>\})}. More info at \url{http://images.tmcnet.com/expo/webrtc-conference/presentations/san-jose-14/D3-2_Testing_v2_2.pdf}}. It is thus probably possible to automate this, but was considered out of scope for this thesis.

Firefox was chosen for these experiments due to its support for user stylesheets, which the Chrome team for some reason has abandoned support for in later versions. appear.in mirrors your own video stream when displayed back to yourself, probably since that's how most people are used to seeing themselves, but this is suboptimal when we want to retain some sanity while manually interpreting numbers shown this way. For browsers with support for user stylesheets, the fix is a simple matter of writing a stylesheet that flips your own image back\cprotect\footnote{For Firefox, this would mean putting the following in \texttt{~/.mozilla/firefox/<user-profile>/chrome/userContent.css}:
\begin{lstlisting}[language=CSS]
@-moz-document domain(appear.in) {
    div.local-client {
        transform: scaleX(-1);
    }
}
\end{lstlisting}}. When working with several machines, having the option to deploy changes like this to all machines with one single file without any other configuration changes is enormously gratifying.


\section{Caveats}

Our method is accurate in the sense that latencies observed are the actual end-to-end latencies that users would observe, but the precision of the timing values observed is not on the millisecond level we'd prefer. This is due to a number of factors, most notably the refresh rate of the screen running the timer and the framerate of the video, limiting the precision to $1s/60\approx17ms$ and $1s/30\approx33ms$ respectively. However, we can surpass this precision by averaging several samples taken during the test run, which is why we take several screenshots for each test case, once every 10s for at least a minute. The standard deviation of the measurements is reported in the graphs included later in this chapter, which should give some indication towards how accurate the average is.

Taking several samples to improve accuracy leads us to another weakness, which is the manual interpretation of the screenshots. Due to the frequency-related issues discussed above, many of the images include timestamps that are blurred, as the camera captured two underlying screen updates in the same frame, as shown in \autoref{fig:blurred-capture}.

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{blurred-capture}
    \caption{A screenshot where a node has sent two overlaying timestamps. In this case interpreted as 10.106, which is reasonable as it's close to a 33ms increase from the previous 10.075.}
    \label{fig:blurred-capture}
\end{figure}

In general for these cases, the recorded timestamp was consistently interpreted to be the latest of what could be distinguished in the screenshot.

Even assuming that the timestamps are comprehensible and fairly accurate, there's still a possibility of human error when lots of numbers has to be recorded in this way. To minimize the risk of any mistyped numbers making it into the dataset, any number outside 1.5 standard deviations of the mean (a range which should include \todo{find actual range covered}80\% of the numbers observed) was verified once more. There's still a chance of smaller errors making it into the dataset, but we assume that these are small enough and distributed evenly among the nodes to not significantly influence any conclusions drawn.

Two different camera models, different processing time, as can be seen in some of the images where other node's video stream arrives before we've processed our own.\todo{Finish this paragraph}

For measuring bandwidth utilization between nodes, our method of using \texttt{tcpdump} is not entirely satisfactory, as there's no way to report actual \emph{consumed} bandwidth by the application. This is due to the traffic control features of the Linux kernel lying above where \texttt{libpcap}, the library that performs packet capture for \texttt{tcpdump}, listens in the network stack, meaning any incoming bandwidth reported will be before the rate limiting performed by \texttt{tc}. Thus, \texttt{tcpdump} cannot report the bandwidth actually consumed by application, only what was actually received by the network interface. Nonetheless, the bandwidth \emph{sent} by each node is what was actually sent by the application, but there's no guarantee that the node was actually capable of consuming it all. This is good enough for us though, as we can aggregate the data sent by all nodes to determine how saturated a given node's network link is.

While the method itself is application-agnostic, configuring nodes the way we do is not suitable for testing other architectures, such as the ones used by Hangouts and Skype. This is unfortunate, as a performance comparison between the different architectures would have been very interesting, but without running a local instance of the architecture under test, there's no way to achieve the internode latencies we desire. This follows from observing that if node A sends her video stream to a Google server, there's no way for her to signal to Google that when the stream is broadcast to nodes B and C, B's stream should be delayed by $x$ms, and the stream to C should be delayed $y$ms. It's also not possible for B and C to apply this latency on the receiving side, as they'd have to split the incoming stream for Google into separate streams for each of the transmitting nodes, which would require both getting access to the DTLS keys used by the web browser to encrypt the traffic, and being capable of splitting the stream and rejoining it again without interfering with the browser.


\section{Method}\label{sec:method}

(screensharing scrapped due to framerate artifically limited too severly, did not run any custom code since the solution needed to be (relatively) service agnostic, and could not modify code since it would need to be tagged in the video stream somehow. Some for appear.in, would need to be included in the sdp stream, and extracted -> way too much work).


\section{Results}

To get a sense of how appear.in performs under optimal conditions, a test run was performed with 3, 4 and 7 nodes in a conversation, without any traffic shaping. We would expect both bandwidth and latencies to be fairly even among all the nodes, as the only thing different between them is the time they entered the conversation. Since they were given some time to stabilize before the measurements was started, some fairness should have been achieved.

As we see in \autoref{fig:vanilla-3p}, bitrates between the nodes are identical, as expected. From the latency measurements there seems to be a slight inclination towards the latest arriving node having somewhat higher latencies outgoing, but the difference is negligible in practice.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C},
            enlargelimits=0.15
            ]
            \input{data/appear.in-vanilla-3p/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C},
            enlargelimits=0.15,
            nodes near coords=\raisebox{.3cm}{\pgfmathprintnumber{\pgfplotspointmeta}}
            ]
            \input{data/appear.in-vanilla-3p/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for three nodes without traffic shaping}
    \label{fig:vanilla-3p}
\end{figure}

Adding another node to the mix, we get the results as shown in \autoref{fig:vanilla-4p}. We see that the bandwidth between the nodes is the same 2.1Mpbs that we saw when there was only three nodes in the conversation, which we can assume is the default transmit bitrate for appear.in. We can also see the same phenomena as in the three node case, that the later arrivals tend to have a slightly higher latency to the nodes first to arrive in the conversation. However, the later nodes (C and D) does not display this phenomena between themselves, thus since all the observations are within one standard deviation of each other we'll not spend any more time investigating this.


\begin{figure}
    \centering
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            bar width=10,
            height=240,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15
            ]
            \input{data/appear.in-vanilla-4p/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C,D},
            bar width=10,
            height=240,
            enlargelimits=0.15,
            nodes near coords=\raisebox{.3cm}{\pgfmathprintnumber{\pgfplotspointmeta}}
            ]
            \input{data/appear.in-vanilla-4p/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for four nodes without traffic shaping}
    \label{fig:vanilla-4p}
\end{figure}

In the final test without traffic shaping, we step it up to seven nodes in the conversation. The results can be seen in \autoref{fig:vanilla-7p}, and largely mirror what we saw in the smaller experiements. Latencies are a bit higher, which can probably be expected due to the increase in traffic, and apart from a bandwidth deviation on node E, all nodes continue transmitting on the previously observed 2.1Mbps. Why node E's behavior is worse than the other nodes are unknown, but it could be related to node E being on a busier subnet than the other nodes (E was the only node in the test cluster on its subnet).


\begin{figure}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            bar width=3,
            height=240,
            symbolic x coords={A,B,C,D,E,F,G},
            enlargelimits=0.10
            ]
            \input{data/appear.in-vanilla-7p/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C,D,E,F,G},
            bar width=3,
            height=240,
            enlargelimits=0.10,
            major grid style=dashed,
            ymajorgrids
            ]
        \input{data/appear.in-vanilla-7p/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for seven nodes without traffic shaping}
    \label{fig:vanilla-7p}
\end{figure}


\todo[inline]{[minor] If time, define custom color set for graphing, based possibly either on solarized or http://clrs.cc/}


For our first test case, "Asia", the results are shown in \autoref{fig:asia-bitrate}. We can tell that even though node B transmits 0.5Mbps to each of the other nodes, neither of them receive recent data frequent enough to keep an acceptable conversation with node B. The good thing is that even though node B suffers greatly in this case, the flow between A and C is unaffected.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            bar width=8,
            symbolic x coords={A,B,C},
            enlargelimits=0.15,
            major grid style=dashed,
            xmajorgrids
            ]
            \input{data/appear.in-asia/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            xtick=data,
            ymax=1000,
            width=\textwidth,
            bar width=8,
            symbolic x coords={A,B,C},
            enlargelimits=0.15,
            major grid style=dashed,
            xmajorgrids,
            nodes near coords=\raisebox{.3cm}{\pgfmathprintnumber{\pgfplotspointmeta}}
            ]
            \input{data/appear.in-asia/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for the "asia" test case. The actual numbers for the out-of-bounds latencies are, from left to right, 25000, 47000, 47000 and 24000.}
    \label{fig:asia-bitrate}
\end{figure}

Next up we have the "standup" test case, as seen in \autoref{fig:standup}. The key challenge in this case node D, with only 6Mbps available on the downlink, slightly upped by node C with 8Mbps. Node C doesn't have any troubles in this test, but node D is completely satured, receiving 2.1Mbps from each of the other three nodes. Even though node D sends to its fullest capacity, hardly anything of this is correctly received by the other nodes. Since there doesn't seem to be any

\begin{figure}
    \centering
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            bar width=8,
            height=240,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15
            ]
            \input{data/appear.in-standup/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \begin{subfigure}[t]{.9\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            ymax=1000,
            bar width=8,
            height=240,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15,
            nodes near coords=\raisebox{.3cm}{\pgfmathprintnumber{\pgfplotspointmeta}}
            ]
            \input{data/appear.in-standup/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for the "standup" test case.}
    \label{fig:standup}
\end{figure}


\begin{figure}
    \centering
    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate (bps),
            xtick=data,
            width=\textwidth,
            bar width=3,
            height=240,
            symbolic x coords={A,B,C,D,E,F,G},
            enlargelimits=0.10
            ]
            \input{data/appear.in-friends/bitrate.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \begin{subfigure}[t]{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            ymax=1000,
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C,D,E,F,G},
            bar width=3,
            height=240,
            enlargelimits=0.10,
            major grid style=dashed,
            ymajorgrids
            ]
            \input{data/appear.in-friends/latency.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for the "friends" test case}
    \label{fig:friends}
\end{figure}


However sobering the Firefox results are, it's unlikely they provide the full story of WebRTC. After all, it's widely deployed and in use by lots of different providers, if this was the expected results it would likely not have gained much traction. Thus, to have a point of comparison, we re-ran the tests using Chrome, but instead of the very manual and labor-intensive method of filming timestamps, used the \texttt{getStats} API to extract the relevant data. This data is provided by the underlying \gls{rtcp}-implementation in the browser.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            bar width=8,
            height=240,
            ymax=150,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15
            ]
            \input{data/appear.in-capture-vanilla-3p/latency-timer.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Latencies measured by filming a timer}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            ymax=150,
            bar width=8,
            height=240,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15,
            ]
            \input{data/appear.in-capture-vanilla-3p/latency-getstats.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Latencies reported by \texttt{getStats}}
    \end{subfigure}
    \caption{Timer broadcasting and getStats compared for three nodes without traffic shaping. Note that the timer broadcasting has only 6 samples, while \texttt{getStats} has 120.}
    \label{fig:standup}
\end{figure}

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            bar width=8,
            height=240,
            ymax=300,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15
            ]
            \input{data/appear.in-capture-asia/latency-timer.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Latencies measured by filming a timer}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Latency (ms),
            xtick=data,
            width=\textwidth,
            ymax=300,
            bar width=8,
            height=240,
            symbolic x coords={A,B,C,D},
            enlargelimits=0.15,
            ]
            \input{data/appear.in-capture-asia/latency-getstats.tex}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Latencies reported by \texttt{getStats}}
    \end{subfigure}
    \caption{Timer broadcasting and getStats compared for the asia test case. Note that the timer broadcasting has only 6 samples, while \texttt{getStats} has 120.}
    \label{fig:standup}
\end{figure}
