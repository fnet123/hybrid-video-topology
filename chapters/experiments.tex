\chapter{Experiments}\label{chp:experiments}

In this chapter the sample implementation is introduced, we'll take a look at the test environment, and present the results.

\section{Test Setup}

Since much of the software to be tested is proprietary and very consumer-oriented, detailed call statistics like dropped packets, latency and bandwidth utilization is not easily available. We need an application-agnostic technique to measure the performance of the different solutions, and our ad-hoc method of doing this is based on comparing timing offsets in the transmitted video stream, and measuring utilized bandwidth as seen by the operating system.

Timing offsets is embedded in the video stream by having each node share their desktop running some pre-recorded video, with an application continously printing a millisecond-accurate clock. Each node can thus find the delay in each stream transmitted to it by finding the difference between it's own clock and the time as transmitted by the other nodes. As long as the clocks on each node is syncronized against the same NTP servers, the clocks should be more than accurate enough to measure the different timing delays observed in the system.

It must be noted, our custom algorithm will not be unjustly favored since it can access the additional forwarding nodes. Google Hangouts and Skype both utilize similar nodes, but their architecture and inner working is hidden from us due to the proprietary nature of these products, and thus they can not be modelled accurately. Only pure peer-to-peer WebRTC products like appear.in and Firefox Hello rely only on the client nodes, but they're both free to add any helpers themselves.


\section{Cost Function}

The cost of utilizing a given network link is a function of the link's utilization, which is given by the classic queuing delay formula, $1/(1 - \mu/\lambda)$. As can be seen in \autoref{fig:utility-latency}, this function is not linear, and can thus not be used directly in our objective function. Nonetheless, we can achieve the same goal -- punishing over-utilization of individual links -- by approximating the function with a piecewise linear graph. The following algorithm partitions a set of slots into a set of edges:

\todo{Insert partitioning-algo here}

Based on this formula, we can formulate the cost multiplier of a link as $1 + k/(\mu - \lambda)$, where $k$ is a customizable parameter for how heavily link saturation should be punished. In our experiments, $k=10$ seems appropriate. This cost function can be seen in \autoref{fig:utility-latency}. We then partion this function into a small set of piecewise linear intervals, which we can model as parallell edges between two nodes, with different capacities and costs. Keeping this set small limits the number of variables in the resulting matrix, and thus keeps processing times reasonably low.

\begin{tikzpicture}
    \begin{axis}[
        xlabel={\% utilization of link},
        ylabel={Cost multiplier},
        xmin=0,
        ymin=1,
        ymax=3,
        xmax=100]

    \addplot+[domain=0:99]{1 + 10/(100 - x)};
    \label{fig:utility-latency}
    \end{axis}
\end{tikzpicture}


\begin{center}
    \label{tab:utilization-to-cost}
    \begin{tabular}{| l | l |}
    \hline
    \textbf{Link utilization} & \textbf{Cost multiplier} \\ \hline
    0--50\% & 1.2 \\ \hline
    51--75\% & 1.40 \\ \hline
    76--80\% & 1.50 \\ \hline
    81--90\% & 2.00 \\ \hline
    \end{tabular}
\end{center}

Using this partitioning as a guide, we can map any number of slots on a physical link into a set of edges $E$ in our graph where $|E| <= 4$.




\section{Implementation}\label{sec:implementation}

\todo{Write about how the implementation was done}


\section{Method}\label{sec:method}

* display latency (can be subtracted)
* display refresh rates (is averaged out (hopefully))
* camera processing times is included in the diff from what's recorded on screen
* chrome devtools echoes browser processing time to be close to ~50ms.

(screensharing scrapped due to framerate artifically limited too severly, did not run any custom code since the solution needed to be (relatively) service agnostic, and could not modify code since it would need to be tagged in the video stream somehow. Some for appear.in, would need to be included in the sdp stream, and extracted -> way too much work).


\section{Results}

\todo[inline]{[minor] If time, define custom color set for graphing, based possibly either on solarized or http://clrs.cc/}
\todo{Gather some metrics from running the implementation in a prod environment}

\begin{tikzpicture}
\begin{axis}[
    ybar,
    ylabel=Bitrate,
    xtick=data,
    symbolic x coords={A,B,C},
    legend style={at={(0.5,-0.15)}, anchor=north,legend columns=-1},
    enlargelimits=0.15
    ]
\addplot+[error bars/.cd,y dir=both, y explicit]
coordinates{
    (A,0) +- (0.0, 0)
    (B,1484609) +- (0.0, 208950)
    (C,1289517) +- (0.0, 488760)};

\addplot+[error bars/.cd,y dir=both, y explicit]
coordinates{
    (A,1441354) +- (0.0, 27516)
    (B,0) +- (0.0, 0)
    (C,1527514) +- (0.0, 369720)};

\addplot+[error bars/.cd,y dir=both, y explicit]
coordinates{
    (A,1446979) +- (0.0, 25559)
    (B,1550429) +- (0.0, 260522)
    (C,0) +- (0.0, 0)};

\legend{A, B, C}
\end{axis}
\end{tikzpicture}


For our first test case, "Asia", the bandwidth utilized by each node is shown insia-bitrate.

\begin{figure}
    \centering
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            ylabel=Bitrate,
            xtick=data,
            width=\textwidth,
            symbolic x coords={A,B,C},
            legend style={at={(0.5,-0.15)}, anchor=north,legend columns=-1},
            enlargelimits=0.15
            ]
            %% Traffic received by A
            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (A,0) +- (0.0, 0)
                (B,499012) +- (0.0, 7163)
                (C,2146920) +- (0.0, 94119)};

            %% Traffic received by B
            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (A,1466045) +- (0.0, 396215)
                (B,0) +- (0.0, 0)
                (C,2128002) +- (0.0, 175433)};

            %% Traffic received by C
            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (A,1847944) +- (0.0, 247449)
                (B,498988) +- (0.0, 5709)
                (C,0) +- (0.0, 0)};
            \legend{A, B, C}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing traffic from each node}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
            ybar,
            compat=newest,
            ylabel=Latency (ms),
            xtick=data,
            ymax=1000,
            width=\textwidth,
            symbolic x coords={A,B,C},
            legend style={at={(0.5,-0.15)}, anchor=north,legend columns=-1},
            enlargelimits=0.15,
            nodes near coords=\raisebox{.3cm}{\pgfmathprintnumber{\pgfplotspointmeta}}
            ]

            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (B,25186) +- (0.0, 7820)
                (C,134) +- (0.0, 106)};

            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (A,47731) +- (0.0, 21209)
                (C,47752) +- (0.0, 21196)};

            \addplot+[error bars/.cd,y dir=both, y explicit]
            coordinates{
                (A,41) +- (0.0, 23)
                (B,24047) +- (0.0, 6977)};
            \legend{A, B, C}
        \end{axis}
        \end{tikzpicture}
        \subcaption{Outgoing latencies for each node}
    \end{subfigure}
    \caption{Test results for the "asia" test case.}
\end{figure}
